{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff085e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported modules.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the examples\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88cd304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc3c1811",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.layers' has no attribute 'HashedCrossing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13156\\3118853345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# Cross the latitude and longitude features into a single one-hot vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m feature_cross = tf.keras.layers.HashedCrossing(\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;31m# num_bins can be adjusted: Higher values improve accuracy, lower values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# improve performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.layers' has no attribute 'HashedCrossing'"
     ]
    }
   ],
   "source": [
    "# Keras Input tensors of float values.\n",
    "inputs = {\n",
    "    'latitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='latitude'),\n",
    "    'longitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='longitude'),\n",
    "    'median_income':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='median_income'),\n",
    "    'population':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='population')\n",
    "}\n",
    "\n",
    "# Create a Normalization layer to normalize the median_income data.\n",
    "median_income = tf.keras.layers.Normalization(\n",
    "    name='normalization_median_income',\n",
    "    axis=None)\n",
    "median_income.adapt(train_df['median_income'])\n",
    "median_income = median_income(inputs.get('median_income'))\n",
    "\n",
    "# Create a Normalization layer to normalize the population data.\n",
    "population = tf.keras.layers.Normalization(\n",
    "    name='normalization_population',\n",
    "    axis=None)\n",
    "population.adapt(train_df['population'])\n",
    "population = population(inputs.get('population'))\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for latitude.\n",
    "# Because we're using a Normalization layer, values for latitude and longitude\n",
    "# will be in the range of approximately -3 to 3 (representing the Z score).\n",
    "# We'll create 20 buckets, which requires 21 bucket boundaries (hence, 20+1).\n",
    "latitude_boundaries = np.linspace(-3, 3, 20+1)\n",
    "\n",
    "# Create a Normalization layer to normalize the latitude data.\n",
    "latitude = tf.keras.layers.Normalization(\n",
    "    name='normalization_latitude',\n",
    "    axis=None)\n",
    "latitude.adapt(train_df['latitude'])\n",
    "latitude = latitude(inputs.get('latitude'))\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(latitude)\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for longitude.\n",
    "longitude_boundaries = np.linspace(-3, 3, 20+1)\n",
    "\n",
    "# Create a Normalization layer to normalize the longitude data.\n",
    "longitude = tf.keras.layers.Normalization(\n",
    "    name='normalization_longitude',\n",
    "    axis=None)\n",
    "longitude.adapt(train_df['longitude'])\n",
    "longitude = longitude(inputs.get('longitude'))\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(longitude)\n",
    "\n",
    "# Cross the latitude and longitude features into a single one-hot vector.\n",
    "# num_bins can be adjusted: Higher values improve accuracy, lower values\n",
    "    # improve performance\n",
    "feature_cross = tf.keras.layers.HashedCrossing(\n",
    "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
    "    output_mode='one_hot',\n",
    "    name='cross_latitude_longitude')([latitude, longitude])\n",
    "\n",
    "# Concatenate our inputs into a single tensor.\n",
    "preprocessing_layers = tf.keras.layers.Concatenate()(\n",
    "    [feature_cross, median_income, population])\n",
    "\n",
    "print(\"Preprocessing layers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83231f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "#Before creating a deep neural net, find a baseline loss by running a simple linear regression model \n",
    "#that uses the preprocessing layers you just created.\n",
    "def plot_the_loss_curve(epochs, mse_training, mse_validation):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse_training, label=\"Training Loss\")\n",
    "  plt.plot(epochs, mse_validation, label=\"Validation Loss\")\n",
    "    \n",
    "  # mse_training is a pandas Series, so convert it to a list first.\n",
    "  merged_mse_lists = mse_training.tolist() + mse_validation\n",
    "  highest_loss = max(merged_mse_lists)\n",
    "  lowest_loss = min(merged_mse_lists)\n",
    "  top_of_y_axis = highest_loss * 1.03\n",
    "  bottom_of_y_axis = lowest_loss * 0.97\n",
    "\n",
    "  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defce69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "def create_model(my_inputs, my_outputs, my_learning_rate):\n",
    "    \n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "      learning_rate=my_learning_rate),\n",
    "      loss=\"mean_squared_error\",\n",
    "      metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name, validation_split=0.1):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = train_median_house_value_normalized(\n",
    "      np.array(features.pop(label_name)))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split)\n",
    "\n",
    "  # Get details that will be useful for plotting the loss curve.\n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse, history.history\n",
    "\n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f434fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define normalized label columns\n",
    "# Create Normalization layers to normalize the median_house_value data.\n",
    "# Because median_house_value is our label (i.e., the target value we're\n",
    "# predicting), these layers won't be added to our model.\n",
    "train_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "train_median_house_value_normalized.adapt(\n",
    "    np.array(train_df['median_house_value']))\n",
    "\n",
    "test_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "test_median_house_value_normalized.adapt(\n",
    "    np.array(test_df['median_house_value']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61aa570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs_linear_regression():\n",
    "    \n",
    "      # Create the Dense output layer.\n",
    "      dense_output = tf.keras.layers.Dense(units=1,\n",
    "                                  name='dense_output')(preprocessing_layers)\n",
    "\n",
    "      # Define an output dictionary we'll send to the model constructor.\n",
    "      outputs = {\n",
    "        'dense_output': dense_output\n",
    "      }\n",
    "      return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c262d7d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13156\\342429035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_outputs_linear_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Establish the model's topography.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13156\\6989058.py\u001b[0m in \u001b[0;36mget_outputs_linear_regression\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[1;31m# Create the Dense output layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       dense_output = tf.keras.layers.Dense(units=1,\n\u001b[1;32m----> 5\u001b[1;33m                                   name='dense_output')(preprocessing_layers)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[1;31m# Define an output dictionary we'll send to the model constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing_layers' is not defined"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 1000\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set.\n",
    "validation_split = 0.2\n",
    "\n",
    "outputs = get_outputs_linear_regression()\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse, history = train_model(my_model, train_df, epochs, batch_size,\n",
    "                          label_name, validation_split)\n",
    "plot_the_loss_curve(epochs, mse, history[\"val_mean_squared_error\"])\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = test_median_house_value_normalized(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs_dnn():\n",
    "    \n",
    "    \n",
    "    # Create a Dense layer with 20 nodes.\n",
    "    dense_output = tf.keras.layers.Dense(units=20,\n",
    "                                         activation='relu',\n",
    "                                         name='hidden_dense_layer_1')(preprocessing_layers)\n",
    "    # Create a Dense layer with 12 nodes.\n",
    "    dense_output = tf.keras.layers.Dense(units=12,\n",
    "                                         activation='relu',\n",
    "                                         name='hidden_dense_layer_2')(dense_output)\n",
    "    # Create the Dense output layer.\n",
    "    dense_output = tf.keras.layers.Dense(units=1,\n",
    "                                         name='dense_output')(dense_output)\n",
    "\n",
    "    # Define an output dictionary we'll send to the model constructor.\n",
    "    outputs = {\n",
    "        'dense_output': dense_output\n",
    "    }\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set.\n",
    "validation_split = 0.2\n",
    "\n",
    "dnn_outputs = get_outputs_dnn()\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(\n",
    "    inputs,\n",
    "    dnn_outputs,\n",
    "    learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined in our inputs.\n",
    "epochs, mse, history = train_model(my_model, train_df, epochs,\n",
    "                                   batch_size, label_name, validation_split)\n",
    "plot_the_loss_curve(epochs, mse, history[\"val_mean_squared_error\"])\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = test_median_house_value_normalized(np.array(test_features.pop(label_name))) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
